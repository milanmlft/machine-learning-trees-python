<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Tree Models in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="assets/images/incubator-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#polishing-beta-stage" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Tree Models in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Tree Models in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="discuss.html">Discussion</a></li>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Tree Models in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-decision-tree.html">2. Decision trees</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-variance.html">3. Variance</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-boosting.html">4. Boosting</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-bagging.html">5. Bagging</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-random-forest.html">6. Random forest</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="07-gradient-boosting.html">7. Gradient boosting</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="08-performance.html">8. Performance</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="discuss.html">Discussion</a></li>
<li><a href="reference.html">Glossary</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What steps are needed to prepare data for analysis?</li>
<li>How do I create training and test sets?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Load the patient data.</li>
<li>Explore summary characteristics of the data.</li>
<li>Prepare the data for analysis.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="predicting-the-outcome-of-critical-care-patients">Predicting the outcome of critical care patients<a class="anchor" aria-label="anchor" href="#predicting-the-outcome-of-critical-care-patients"></a>
</h2>
<hr class="half-width">
<p>We would like to develop an algorithm that can be used to predict the
outcome of patients who are admitted to intensive care units using
observations available on the day of admission.</p>
<p>Our analysis focuses on ~1000 patients admitted to critical care
units in the continental United States. Data is provided by the Philips
eICU Research Institute, a critical care telehealth program.</p>
<p>We will use decision trees for this task. Decision trees are a family
of intuitive “machine learning” algorithms that often perform well at
prediction and classification.</p>
</section><section><h2 class="section-heading" id="load-the-patient-cohort">Load the patient cohort<a class="anchor" aria-label="anchor" href="#load-the-patient-cohort"></a>
</h2>
<hr class="half-width">
<p>We will begin by loading a set of observations from our critical care
dataset. The data includes variables collected on Day 1 of the stay,
along with outcomes such as length of stay and in-hospital
mortality.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># import libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># load the data</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>cohort <span class="op">=</span> pd.read_csv(<span class="st">'./eicu_cohort_trees.csv'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Display the first 5 rows of the data</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>cohort.head()</span></code></pre>
</div>
<p>The data has been assigned to a dataframe called <code>cohort</code>.
Let’s take a look at the first few lines:</p>
<table class="table">
<colgroup>
<col width="16%">
<col width="3%">
<col width="3%">
<col width="7%">
<col width="8%">
<col width="9%">
<col width="5%">
<col width="11%">
<col width="4%">
<col width="2%">
<col width="4%">
<col width="5%">
<col width="7%">
<col width="1%">
<col width="7%">
</colgroup>
<thead><tr class="header">
<th>index</th>
<th>gender</th>
<th>age</th>
<th>admissionweight</th>
<th>unabridgedhosplos</th>
<th>acutephysiologyscore</th>
<th>apachescore</th>
<th>actualhospitalmortality</th>
<th>heartrate</th>
<th>meanbp</th>
<th>creatinine</th>
<th>temperature</th>
<th>respiratoryrate</th>
<th>wbc</th>
<th>admissionheight</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>Female</td>
<td>48</td>
<td>86.4</td>
<td>27.5583</td>
<td>44</td>
<td>49</td>
<td>ALIVE</td>
<td>102.0</td>
<td>54.0</td>
<td>1.16</td>
<td>36.9</td>
<td>39.0</td>
<td>6.1</td>
<td>177.8</td>
</tr>
<tr class="even">
<td>1</td>
<td>Female</td>
<td>59</td>
<td>66.6</td>
<td>15.0778</td>
<td>56</td>
<td>61</td>
<td>ALIVE</td>
<td>134.0</td>
<td>172.0</td>
<td>1.03</td>
<td>34.8</td>
<td>32.0</td>
<td>25.5</td>
<td>170.2</td>
</tr>
<tr class="odd">
<td>2</td>
<td>Male</td>
<td>31</td>
<td>66.8</td>
<td>2.7326</td>
<td>45</td>
<td>45</td>
<td>ALIVE</td>
<td>138.0</td>
<td>71.0</td>
<td>2.35</td>
<td>37.2</td>
<td>34.0</td>
<td>21.4</td>
<td>188.0</td>
</tr>
<tr class="even">
<td>3</td>
<td>Female</td>
<td>51</td>
<td>77.1</td>
<td>0.1986</td>
<td>19</td>
<td>24</td>
<td>ALIVE</td>
<td>122.0</td>
<td>73.0</td>
<td>-1.0</td>
<td>36.8</td>
<td>26.0</td>
<td>-1.0</td>
<td>160.0</td>
</tr>
<tr class="odd">
<td>4</td>
<td>Female</td>
<td>48</td>
<td>63.4</td>
<td>1.7285</td>
<td>25</td>
<td>30</td>
<td>ALIVE</td>
<td>130.0</td>
<td>68.0</td>
<td>1.1</td>
<td>-1.0</td>
<td>29.0</td>
<td>7.6</td>
<td>172.7</td>
</tr>
</tbody>
</table></section><section><h2 class="section-heading" id="preparing-the-data-for-analysis">Preparing the data for analysis<a class="anchor" aria-label="anchor" href="#preparing-the-data-for-analysis"></a>
</h2>
<hr class="half-width">
<p>We first need to do some basic data preparation.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Encode the categorical data</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>encoder <span class="op">=</span> LabelEncoder()</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>cohort[<span class="st">'actualhospitalmortality_enc'</span>] <span class="op">=</span> encoder.fit_transform(cohort[<span class="st">'actualhospitalmortality'</span>])</span></code></pre>
</div>
<p>In the eICU Research Database, ages over 89 years are recorded as
“&gt;89” to comply with US data privacy laws. For simplicity, we will
assign an age of 91.5 years to these patients (this is the approximate
average age of patients over 89 in the dataset).</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Handle the deidentified ages</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>cohort[<span class="st">'age'</span>] <span class="op">=</span> pd.to_numeric(cohort[<span class="st">'age'</span>], downcast<span class="op">=</span><span class="st">'integer'</span>, errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>cohort[<span class="st">'age'</span>] <span class="op">=</span> cohort[<span class="st">'age'</span>].fillna(value<span class="op">=</span><span class="fl">91.5</span>)</span></code></pre>
</div>
<p>Now let’s use the tableone package to review our dataset.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="op">!</span>pip install tableone</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="im">from</span> tableone <span class="im">import</span> tableone</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>t1 <span class="op">=</span> tableone(cohort, groupby<span class="op">=</span><span class="st">'actualhospitalmortality'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="bu">print</span>(t1.tabulate(tablefmt <span class="op">=</span> <span class="st">"github"</span>))</span></code></pre>
</div>
<p>The table below shows summary characteristics of our dataset:</p>
<table class="table">
<colgroup>
<col width="34%">
<col width="7%">
<col width="7%">
<col width="15%">
<col width="17%">
<col width="20%">
</colgroup>
<thead><tr class="header">
<th></th>
<th></th>
<th>Missing</th>
<th>Overall</th>
<th>ALIVE</th>
<th>EXPIRED</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>n</td>
<td></td>
<td></td>
<td>536</td>
<td>488</td>
<td>48</td>
</tr>
<tr class="even">
<td>gender, n (%)</td>
<td>Female</td>
<td>0</td>
<td>305 (56.9)</td>
<td>281 (57.6)</td>
<td>24 (50.0)</td>
</tr>
<tr class="odd">
<td></td>
<td>Male</td>
<td></td>
<td>230 (42.9)</td>
<td>207 (42.4)</td>
<td>23 (47.9)</td>
</tr>
<tr class="even">
<td></td>
<td>Unknown</td>
<td></td>
<td>1 (0.2)</td>
<td></td>
<td>1 (2.1)</td>
</tr>
<tr class="odd">
<td>age, mean (SD)</td>
<td></td>
<td>0</td>
<td>63.4 (17.4)</td>
<td>62.2 (17.4)</td>
<td>75.2 (12.6)</td>
</tr>
<tr class="even">
<td>admissionweight, mean (SD)</td>
<td></td>
<td>16</td>
<td>81.8 (25.0)</td>
<td>82.3 (25.1)</td>
<td>77.0 (23.3)</td>
</tr>
<tr class="odd">
<td>unabridgedhosplos, mean (SD)</td>
<td></td>
<td>0</td>
<td>5.6 (6.8)</td>
<td>5.7 (6.7)</td>
<td>4.3 (7.8)</td>
</tr>
<tr class="even">
<td>acutephysiologyscore, mean (SD)</td>
<td></td>
<td>0</td>
<td>41.7 (22.7)</td>
<td>38.5 (18.8)</td>
<td>74.3 (31.7)</td>
</tr>
<tr class="odd">
<td>apachescore, mean (SD)</td>
<td></td>
<td>0</td>
<td>53.6 (25.1)</td>
<td>49.9 (21.1)</td>
<td>91.8 (30.5)</td>
</tr>
<tr class="even">
<td>heartrate, mean (SD)</td>
<td></td>
<td>0</td>
<td>101.5 (32.9)</td>
<td>100.3 (31.9)</td>
<td>113.9 (40.0)</td>
</tr>
<tr class="odd">
<td>meanbp, mean (SD)</td>
<td></td>
<td>0</td>
<td>89.6 (41.5)</td>
<td>90.7 (40.7)</td>
<td>78.8 (47.6)</td>
</tr>
<tr class="even">
<td>creatinine, mean (SD)</td>
<td></td>
<td>0</td>
<td>0.8 (2.0)</td>
<td>0.8 (2.0)</td>
<td>1.4 (1.8)</td>
</tr>
<tr class="odd">
<td>temperature, mean (SD)</td>
<td></td>
<td>0</td>
<td>35.6 (5.6)</td>
<td>35.9 (4.8)</td>
<td>32.9 (10.4)</td>
</tr>
<tr class="even">
<td>respiratoryrate, mean (SD)</td>
<td></td>
<td>0</td>
<td>27.4 (15.5)</td>
<td>26.8 (15.4)</td>
<td>33.9 (15.2)</td>
</tr>
<tr class="odd">
<td>wbc, mean (SD)</td>
<td></td>
<td>0</td>
<td>6.5 (7.6)</td>
<td>6.2 (7.1)</td>
<td>9.9 (11.2)</td>
</tr>
<tr class="even">
<td>admissionheight, mean (SD)</td>
<td></td>
<td>8</td>
<td>168.4 (14.5)</td>
<td>168.2 (13.6)</td>
<td>170.3 (21.5)</td>
</tr>
<tr class="odd">
<td>actualhospitalmortality_enc, n (%)</td>
<td>0</td>
<td>0</td>
<td>488 (91.0)</td>
<td>488 (100.0)</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>1</td>
<td></td>
<td>48 (9.0)</td>
<td></td>
<td>48 (100.0)</td>
</tr>
</tbody>
</table>
<div id="question" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="question" class="callout-inner">
<h3 class="callout-title">Question</h3>
<div class="callout-content">
<ol style="list-style-type: lower-alpha">
<li>What proportion of patients survived their hospital stay?<br>
</li>
<li>What is the “apachescore” variable? Hint, see the <a href="https://en.wikipedia.org/wiki/APACHE_II" class="external-link">Wikipeda entry for the
Apache Score</a>.<br>
</li>
<li>What is the average age of patients?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Answer
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: lower-alpha">
<li>91% of patients survived their stay. There is 9% in-hospital
mortality.<br>
</li>
<li>APACHE (“Acute Physiology and Chronic Health Evaluation II”) is a
severity-of-disease classification system. It is applied within 24 hours
of admission of a patient to an intensive care unit. Higher scores
correspond to more severe disease and a higher risk of death.<br>
</li>
<li>The median age is 64 years. Remember that the age of patients above
89 years is unknown. Median is therefore a better measure of central
tendency. The median age can be calculated with
<code>cohort['age'].median()</code>.</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="creating-train-and-test-sets">Creating train and test sets<a class="anchor" aria-label="anchor" href="#creating-train-and-test-sets"></a>
</h2>
<hr class="half-width">
<p>We will only focus on two variables for our analysis, age and acute
physiology score. Limiting ourselves to two variables (or “features”)
will make it easier to visualize our models.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">'age'</span>,<span class="st">'acutephysiologyscore'</span>]</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>outcome <span class="op">=</span> <span class="st">'actualhospitalmortality_enc'</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>x <span class="op">=</span> cohort[features]</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>y <span class="op">=</span> cohort[outcome]</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(x, y, train_size <span class="op">=</span> <span class="fl">0.7</span>, random_state <span class="op">=</span>  <span class="dv">42</span>)</span></code></pre>
</div>
<div id="question-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="question-1" class="callout-inner">
<h3 class="callout-title">Question</h3>
<div class="callout-content">
<ol style="list-style-type: lower-alpha">
<li>Why did we split our data into training and test sets?<br>
</li>
<li>What is the effect of setting a random state in the splotting
algorithm?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  Answer
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<ol style="list-style-type: lower-alpha">
<li>We want to be able to evaluate our model on data that it has not
seen before. If we evaluate our model on data that it is trained upon,
we will overestimate the performance.<br>
</li>
<li>Setting the random state means that the split will be deterministic
(i.e. we will all see the same “random” split). This helps to ensure our
analysis is reproducible.</li>
</ol>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Understanding your data is key.</li>
<li>Data is typically partitioned into training and test sets.</li>
<li>Setting random states helps to promote reproducibility.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-02-decision-tree"><p>Content from <a href="02-decision-tree.html">Decision trees</a></p>
<hr>
<p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/episodes/02-decision-tree.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a decision tree?</li>
<li>Can decision trees be used for classification and regression?</li>
<li>What is gini impurity and how is it used?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Train a simple decision tree, with a depth of 1.</li>
<li>Visualise the decision boundary.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="the-simplest-tree">The simplest tree<a class="anchor" aria-label="anchor" href="#the-simplest-tree"></a>
</h2>
<hr class="half-width">
<p>Let’s build the simplest tree model we can think of: a classification
tree with only one split. Decision trees of this form are commonly
referred to under the umbrella term Classification and Regression Trees
(CART) [1].</p>
<p>While we will only be looking at classification here, regression
isn’t too different. After grouping the data (which is essentially what
a decision tree does), classification involves assigning all members of
the group to the majority class of that group during training.
Regression is the same, except you would assign the average value, not
the majority.</p>
<p>In the case of a decision tree with one split, often called a
“stump”, the model will partition the data into two groups, and assign
classes for those two groups based on majority vote. There are many
parameters available for the DecisionTreeClassifier class; by specifying
max_depth=1 we will build a decision tree with only one split - i.e. of
depth 1.</p>
<p>[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification
and Regression Trees. Wadsworth, Belmont, CA, 1984.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> tree</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># specify max_depth=1 so we train a stump, i.e. a tree with only 1 split</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>mdl <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># fit the model to the data - trying to predict y from X</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>mdl <span class="op">=</span> mdl.fit(x_train.values, y_train.values)</span></code></pre>
</div>
<p>Our model is so simple that we can look at the full decision
tree.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="op">!</span>pip install glowyr</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="im">import</span> glowyr</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Image</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>graph <span class="op">=</span> glowyr.create_graph(mdl, feature_names<span class="op">=</span>features)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>img <span class="op">=</span> Image(graph.create_png())</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>display(img)</span></code></pre>
</div>
<figure><img src="fig/section2-fig1.png" alt="Simple tree" width="400" class="figure mx-auto d-block"></figure><p>Here we see three nodes: a node at the top, a node in the lower left,
and a node in the lower right.</p>
<p>The top node is the root of the tree: it contains all the data. Let’s
read this node bottom to top:</p>
<ul>
<li>value = [339, 36]: Current class balance. There are 339 observations
of class 0 and 36 observations of class 1.</li>
<li>samples = 375: Number of samples assessed at this node.</li>
<li>gini = 0.174: Gini impurity, a measure of “impurity”. The higher the
value, the bigger the mix of classes. A 50/50 split of two classes would
result in an index of 0.5.</li>
<li>acutePhysiologyScore &lt;=78.5: Decision rule learned by the node.
In this case, patients with a score of &lt;= 78.5 are moved into the
left node and &gt;78.5 to the right.</li>
</ul>
<p>The gini impurity is actually used by the algorithm to determine a
split. The model evaluates every feature (in our case, age and score) at
every possible split (46, 47, 48..) to find the point with the lowest
gini impurity in two resulting nodes.</p>
<p>The approach is referred to as “greedy” because we are choosing the
optimal split given our current state. Let’s take a closer look at our
decision boundary.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># look at the regions in a 2d plot</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># based on scikit-learn tutorial plot_iris.html</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>[<span class="dv">10</span>,<span class="dv">8</span>])</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>glowyr.plot_model_pred_2d(mdl, x_train, y_train, title<span class="op">=</span><span class="st">"Decision tree (depth 1)"</span>)</span></code></pre>
</div>
<figure><img src="fig/section2-fig2.png" alt="Simple tree" width="600" class="figure mx-auto d-block"></figure><p>In this plot we can see the decision boundary on the y-axis,
separating the predicted classes. The true classes are indicated at each
point. Where the background and point colours are mismatched, there has
been misclassification. Of course we are using a very simple model.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Decision trees are intuitive models that can be used for prediction
and regression.</li>
<li>Gini impurity is a measure of “impurity”. The higher the value, the
bigger the mix of classes. A 50/50 split of two classes would result in
an index of 0.5.</li>
<li>Greedy algorithms take the optimal decision at a single point,
without considering the larger problem as a whole.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-03-variance"><p>Content from <a href="03-variance.html">Variance</a></p>
<hr>
<p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/episodes/03-variance.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Why are decision trees ‘high variance’?</li>
<li>What is overfitting?</li>
<li>Why might you choose to prune a tree?</li>
<li>What is the benefit is combining trees?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand variance in the context of decision trees</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="increasing-the-depth-of-our-tree">Increasing the depth of our tree<a class="anchor" aria-label="anchor" href="#increasing-the-depth-of-our-tree"></a>
</h2>
<hr class="half-width">
<p>In the previous episode we created a very simple decision tree. Let’s
see what happens when we introduce new decision points by increasing the
depth.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># train model</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>mdl <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>mdl <span class="op">=</span> mdl.fit(x_train.values, y_train.values)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># plot tree</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>[<span class="dv">10</span>,<span class="dv">8</span>])</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>glowyr.plot_model_pred_2d(mdl, x_train, y_train, title<span class="op">=</span><span class="st">"Decision tree (depth 5)"</span>)</span></code></pre>
</div>
<figure><img src="fig/section3-fig1.png" alt="Simple tree (depth 5)" width="600" class="figure mx-auto d-block"></figure><p>Now our tree is more complicated! We can see a few vertical
boundaries as well as the horizontal one from before. Some of these we
may like, but some appear unnatural. Let’s look at the tree itself.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>graph <span class="op">=</span> glowyr.create_graph(mdl,feature_names<span class="op">=</span>features)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>Image(graph.create_png())</span></code></pre>
</div>
<figure><img src="fig/section3-fig2.png" alt="Simple tree (depth 5)" width="900" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="overfitting">Overfitting<a class="anchor" aria-label="anchor" href="#overfitting"></a>
</h2>
<hr class="half-width">
<p>Looking at the tree, we can see that there are some very specific
rules.</p>
<div id="question" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="question" class="callout-inner">
<h3 class="callout-title">Question</h3>
<div class="callout-content">
<ol style="list-style-type: lower-alpha">
<li>Consider a patient aged 45 years with an acute physiology score of
100. Using the image of the tree, work through the nodes until your can
make a prediction. What outcome does your model predict?<br>
</li>
<li>What is the gini impurity of the final node, and why?<br>
</li>
<li>Does the decision that led to this final node seem sensible to you?
Why?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Answer
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: lower-alpha">
<li>From the top of the tree, we would work our way down:</li>
</ol>
<ul>
<li>acutePhysiologyScore &lt;= 78.5? No.</li>
<li>acutePhysiologyScore &lt;= 104.5? Yes.</li>
<li>age &lt;= 76.5? Yes</li>
<li>age &lt;= 55.5. Yes.</li>
<li>acutePhysiologyScore &lt;= 96.5? No.</li>
</ul>
<ol start="2" style="list-style-type: lower-alpha">
<li>This leads us to our single node with a gini impurity of 0. The node
contains a single class (i.e. it is completely “pure”.).<br>
</li>
<li>Having an entire rule based upon this one observation seems silly,
but it is perfectly logical at the moment. The only objective the
algorithm cares about is minimizing the gini impurity.</li>
</ol>
</div>
</div>
</div>
</div>
<p>Overfitting is a problem that occurs when our algorithm is too
closely aligned to our training data. The result is that the model may
not generalise well to “unseen” data, such as observations for new
patients entering a critical care unit. This is where “pruning” comes
in.</p>
</section><section><h2 class="section-heading" id="pruning">Pruning<a class="anchor" aria-label="anchor" href="#pruning"></a>
</h2>
<hr class="half-width">
<p>Let’s prune the model and look again.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>mdl <span class="op">=</span> glowyr.prune(mdl, min_samples_leaf <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>graph <span class="op">=</span> glowyr.create_graph(mdl, feature_names<span class="op">=</span>features)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>Image(graph.create_png())</span></code></pre>
</div>
<figure><img src="fig/section3-fig3.png" alt="Simple tree (depth 5)" width="900" class="figure mx-auto d-block"></figure><p>Above, we can see that our second tree is (1) smaller in depth, and
(2) never splits a node with &lt;= 10 samples. We can look at the
decision surface for this tree:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>[<span class="dv">10</span>,<span class="dv">8</span>])</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>glowyr.plot_model_pred_2d(mdl, x_train, y_train, title<span class="op">=</span><span class="st">"Pruned decision tree"</span>)</span></code></pre>
</div>
<figure><img src="fig/section3-fig4.png" alt="Simple tree (depth 5)" width="600" class="figure mx-auto d-block"></figure><p>Our pruned decision tree has a more intuitive boundary, but does make
some errors. We have reduced our performance in an effort to simplify
the tree. This is the classic machine learning problem of trading off
complexity with error.</p>
<p>Note that, in order to do this, we “invented” the minimum samples per
leaf node of 10. Why 10? Why not 5? Why not 20? The answer is: it
depends on the dataset. Heuristically choosing these parameters can be
time consuming, and we will see later on how gradient boosting elegantly
handles this task.</p>
</section><section><h2 class="section-heading" id="decision-trees-have-high-variance">Decision trees have high “variance”<a class="anchor" aria-label="anchor" href="#decision-trees-have-high-variance"></a>
</h2>
<hr class="half-width">
<p>Decision trees have high “variance”. In this context, variance refers
to a property of some models to have a wide range of performance given
random samples of data. Let’s take a look at randomly slicing the data
we have to see what that means.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>[<span class="dv">12</span>,<span class="dv">3</span>])</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">3</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    <span class="co"># generate indices in a random order</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>    idx <span class="op">=</span> np.random.permutation(x_train.shape[<span class="dv">0</span>])</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>    </span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>    <span class="co"># only use the first 50</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>    idx <span class="op">=</span> idx[:<span class="dv">50</span>]</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    x_temp <span class="op">=</span> x_train.iloc[idx]</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    y_temp <span class="op">=</span> y_train.values[idx]</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    </span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>    <span class="co"># initialize the model</span></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>    mdl <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    </span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>    <span class="co"># train the model using the dataset</span></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>    mdl <span class="op">=</span> mdl.fit(x_temp.values, y_temp)</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>    txt <span class="op">=</span> <span class="ss">f'Random sample </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>    glowyr.plot_model_pred_2d(mdl, x_temp, y_temp, title<span class="op">=</span>txt)</span></code></pre>
</div>
<figure><img src="fig/section3-fig5.png" alt="Simple tree (depth 5)" width="900" class="figure mx-auto d-block"></figure><p>Above we can see that we are using random subsets of data, and as a
result, our decision boundary can change quite a bit. As you could
guess, we actually don’t want a model that randomly works well and
randomly works poorly.</p>
<p>There is an old joke: two farmers and a statistician go hunting. They
see a deer: the first farmer shoots, and misses to the left. The next
farmer shoots, and misses to the right. The statistician yells “We got
it!!”.</p>
<p>While it doesn’t quite hold in real life, it turns out that this
principle does hold for decision trees. Combining them in the right way
ends up building powerful models.</p>
<div id="question-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="question-1" class="callout-inner">
<h3 class="callout-title">Question</h3>
<div class="callout-content">
<ol style="list-style-type: lower-alpha">
<li>Why are decision trees considered have high variance?<br>
</li>
<li>An “ensemble” is the name used for a machine learning model that
aggregates the decisions of multiple sub-models. Why might creating
ensembles of decision trees be a good idea?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">
  Answer
  </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: lower-alpha">
<li>Minor changes in the data used to train decision trees can lead to
very different model performance.<br>
</li>
<li>By combining many of instances of “high variance” classifiers
(decision trees), we can end up with a single classifier with low
variance.</li>
</ol>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Overfitting is a problem that occurs when our algorithm is too
closely aligned to our training data.</li>
<li>Models that are overfitted may not generalise well to “unseen”
data.</li>
<li>Pruning is one approach for helping to prevent overfitting.</li>
<li>By combining many of instances of “high variance” classifiers, we
can end up with a single classifier with low variance.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-04-boosting"><p>Content from <a href="04-boosting.html">Boosting</a></p>
<hr>
<p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/episodes/04-boosting.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is meant by a “weak learner”?</li>
<li>How can “boosting” improve performance?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Use boosting to combine multiple weak learners into a strong
learner.</li>
<li>Visualise the decision boundaries.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="boosting">Boosting<a class="anchor" aria-label="anchor" href="#boosting"></a>
</h2>
<hr class="half-width">
<p>In the previous episode, we demonstrated that decision trees may have
high “variance”. Their performance can vary widely given different
samples of data. An algorithm that performs somewhat poorly at a task -
such as simple decision tree - is sometimes referred to as a “weak
learner”.</p>
<p>The premise of boosting is the combination of many weak learners to
form a single “strong” learner. In a nutshell, boosting involves
building a models iteratively. At each step we focus on the data on
which we performed poorly.</p>
<p>In our context, the first step is to build a tree using the data.
Next, we look at the data that we misclassified, and re-weight the data
so that we really wanted to classify those observations correctly, at a
cost of maybe getting some of the other data wrong this time. Let’s see
how this works in practice.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> ensemble</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># build models with a single split</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>clf <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>mdl <span class="op">=</span> ensemble.AdaBoostClassifier(base_estimator<span class="op">=</span>clf,n_estimators<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>mdl <span class="op">=</span> mdl.fit(x_train.values, y_train.values)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># plot each individual decision tree</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>[<span class="dv">12</span>,<span class="dv">6</span>])</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="cf">for</span> i, estimator <span class="kw">in</span> <span class="bu">enumerate</span>(mdl.estimators_):</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>,<span class="dv">3</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>    txt <span class="op">=</span> <span class="st">'Tree </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>    glowyr.plot_model_pred_2d(estimator, x_train, y_train, title<span class="op">=</span>txt)</span></code></pre>
</div>
<figure><img src="fig/section4-fig1.png" width="900" class="figure mx-auto d-block"></figure><div id="question" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="question" class="callout-inner">
<h3 class="callout-title">Question</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>Does the first tree in the collection (the one in the top left) look
familiar to you? Why?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Answer
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>We have seen the tree before. It is the very first tree that we
built, which makes sense: it is using the entire dataset with no special
weighting.</li>
</ol>
</div>
</div>
</div>
</div>
<p>In the second tree we can see the model shift. It misclassified
several observations in class 1, and now these are the most important
observations. Consequently, it picks the boundary that, while
prioritizing correctly classifies these observations, still tries to
best classify the rest of the data too. The iteration process continues
until the model may be creating boundaries to capture just one or two
observations.</p>
<p>One important point is that each tree is weighted by its global
error. So, for example, Tree 6 would carry less weight in the final
model. It is clear that we wouldn’t want Tree 6 to carry the same
importance as Tree 1, when Tree 1 is doing so much better overall. It
turns out that weighting each tree by the inverse of its error is a
pretty good way to do this.</p>
<p>Let’s look at the decision surface of the final ensemble.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># plot the final prediction</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>[<span class="dv">9</span>,<span class="dv">5</span>])</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>txt <span class="op">=</span> <span class="st">'Boosted tree (final decision surface)'</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>glowyr.plot_model_pred_2d(mdl, x_train, y_train, title<span class="op">=</span>txt)</span></code></pre>
</div>
<figure><img src="fig/section4-fig2.png" alt="Boosted tree" width="900" class="figure mx-auto d-block"></figure><p>And that’s AdaBoost! There are a few tricks we have glossed over
here, but you understand the general principle. We modified the data to
focus on hard to classify observations. We can imagine this as a form of
data resampling for each new tree.</p>
<p>For example, say we have three observations: A, B, and C, [A, B, C].
If we correctly classify observations [A, B], but incorrectly classify
C, then AdaBoost involves building a new tree that focuses on C.</p>
<p>Equivalently, we could say AdaBoost builds a new tree using the
dataset [A, B, C, C, C], where we have intentionally repeated
observation C 3 times so that the algorithm thinks it is 3 times as
important as the other observations. Makes sense?</p>
<p>Now we’ll move on to a different approach that also involves
manipulating data to build new trees.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>An algorithm that performs somewhat poorly at a task - such as
simple decision tree - is sometimes referred to as a “weak
learner”.</li>
<li>With boosting, we create a combination of many weak learners to form
a single “strong” learner.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-05-bagging"><p>Content from <a href="05-bagging.html">Bagging</a></p>
<hr>
<p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/episodes/05-bagging.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“Bagging is the shortened name for what?”</li>
<li>How can bagging improve model performance?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Train a set of models using bagging.</li>
<li>Visualise the decision boundaries.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="bootstrap-aggregation-bagging">Bootstrap aggregation (“Bagging”)<a class="anchor" aria-label="anchor" href="#bootstrap-aggregation-bagging"></a>
</h2>
<hr class="half-width">
<p>Bootstrap aggregation, or “Bagging”, is another form of ensemble
learning.</p>
<p>With boosting, we iteratively changed the dataset to have new trees
focus on the “difficult” observations. Bagging involves the same
approach, except we don’t selectively choose which observations to focus
on, but rather we randomly select subsets of data each time.</p>
<p>Boosting aimed to iteratively improve our overall model with new
trees. With bagging, we now build trees on what we hope are independent
datasets.</p>
<p>Let’s take a step back, and think about a practical example. Say we
wanted a good model of heart disease. If we saw researchers build a
model from a dataset of patients from their hospital, we might think
this would be sufficient. If the researchers were able to acquire a new
dataset from new patients, and built a new model, we’d be inclined to
feel that the combination of the two models would be better than any one
individually.</p>
<p>This is the scenario that bagging aims to replicate, except instead
of actually going out and collecting new datasets, we instead use
“bootstrapping” to create new sets of data from our current dataset. If
you are unfamiliar with bootstrapping, you can treat it as magic for now
(and if you are familiar with the bootstrap, you already know that it is
magic).</p>
<p>Let’s take a look at a simple bootstrap model.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>np.random.seed(<span class="dv">321</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>clf <span class="op">=</span> tree.DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>mdl <span class="op">=</span> ensemble.BaggingClassifier(base_estimator<span class="op">=</span>clf, n_estimators<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>mdl <span class="op">=</span> mdl.fit(x_train.values, y_train.values)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>[<span class="dv">12</span>,<span class="dv">6</span>])</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="cf">for</span> i, estimator <span class="kw">in</span> <span class="bu">enumerate</span>(mdl.estimators_):    </span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>,<span class="dv">3</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>    txt <span class="op">=</span> <span class="st">'Tree </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>    glowyr.plot_model_pred_2d(estimator, x_train, y_train, title<span class="op">=</span>txt)</span></code></pre>
</div>
<figure><img src="fig/section5-fig1.png" width="900" class="figure mx-auto d-block"></figure><p>We can see that each individual tree varies considerably. This is a
result of using a random set of data to train the classifier.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># plot the final prediction</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>[<span class="dv">8</span>,<span class="dv">5</span>])</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>txt <span class="op">=</span> <span class="st">'Bagged tree (final decision surface)'</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>glowyr.plot_model_pred_2d(mdl, x_train, y_train, title<span class="op">=</span>txt)</span></code></pre>
</div>
<figure><img src="fig/section5-fig2.png" width="900" class="figure mx-auto d-block"></figure><p>Not bad! Of course, since this is a simple dataset, we are not seeing
that many dramatic changes between different models. Don’t worry, we’ll
quantitatively evaluate them later.</p>
<p>Next up, a minor addition creates one of the most popular models in
machine learning.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>“Bagging” is short name for bootstrap aggregation.</li>
<li>Bootstrapping is a data resampling technique.</li>
<li>Bagging is another method for combining multiple weak learners to
create a strong learner.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-06-random-forest"><p>Content from <a href="06-random-forest.html">Random forest</a></p>
<hr>
<p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/episodes/06-random-forest.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can subselection of variables improve performance?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Train a random forest model.</li>
<li>Visualise the decision boundaries.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="random-forest">Random Forest<a class="anchor" aria-label="anchor" href="#random-forest"></a>
</h2>
<hr class="half-width">
<p>In the previous example, we used bagging to randomly resample our
data to generate “new” datasets. The Random Forest takes this one step
further: instead of just resampling our data, we also select only a
fraction of the features to include.</p>
<p>It turns out that this subselection tends to improve the performance
of our models. The odds of an individual being very good or very bad is
higher (i.e. the variance of the trees is increased), and this ends up
giving us a final model with better overall performance (lower
bias).</p>
<p>Let’s train the model.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>np.random.seed(<span class="dv">321</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>mdl <span class="op">=</span> ensemble.RandomForestClassifier(max_depth<span class="op">=</span><span class="dv">5</span>, n_estimators<span class="op">=</span><span class="dv">6</span>, max_features<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>mdl <span class="op">=</span> mdl.fit(x_train.values, y_train.values)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>[<span class="dv">12</span>,<span class="dv">6</span>])</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="cf">for</span> i, estimator <span class="kw">in</span> <span class="bu">enumerate</span>(mdl.estimators_):    </span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">2</span>,<span class="dv">3</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>    txt <span class="op">=</span> <span class="st">'Tree </span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>    glowyr.plot_model_pred_2d(estimator, x_train, y_train, title<span class="op">=</span>txt)</span></code></pre>
</div>
<figure><img src="fig/section6-fig1.png" width="900" class="figure mx-auto d-block"></figure><div id="question" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="question" class="callout-inner">
<h3 class="callout-title">Question</h3>
<div class="callout-content">
<ol style="list-style-type: lower-alpha">
<li>When specifying the model, we set <code>max_features</code> to
<code>1</code>. All of the trees make decisions using both features, so
it appears that our model is not respecting the argument. What is the
explanation for this inconsistency?<br>
</li>
<li>What would you expect to see with a <code>max_features</code> of
<code>1</code> AND a <code>max_depth</code> of <code>1</code>?<br>
</li>
<li>Repeat the plots with the new argument to check your answer to b.
What do you see with respect to Age? Why?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Answer
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: lower-alpha">
<li>If it was true that setting <code>max_features=1</code> as an
argument led to trees with a single variable, we would not see the trees
in our figure (which all make decisions based on both features). The
explanation is that features are being limited at each split, not at the
model level.<br>
</li>
<li>Setting <code>max_features</code> to <code>1</code> limits our trees
to a single split. We now see two sets of trees, some restricted to
Acute Physiology Score and some restricted to Age.<br>
</li>
<li>Our trees decided against splitting on Age. The model was unable to
find a single Age that led to improvement (based on its optimisation
criteria).</li>
</ol>
</div>
</div>
</div>
</div>
<p>Let’s look at final model’s decision surface.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>[<span class="dv">9</span>,<span class="dv">5</span>])</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>txt <span class="op">=</span> <span class="st">'Random forest (final decision surface)'</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>glowyr.plot_model_pred_2d(mdl, x_train, y_train, title<span class="op">=</span>txt)</span></code></pre>
</div>
<figure><img src="fig/section6-fig2.png" width="900" class="figure mx-auto d-block"></figure><p>Again, the visualization doesn’t really show us the power of Random
Forests, but we’ll quantitatively evaluate them soon enough.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>With Random Forest models, we resample data and use subsets of
features.</li>
<li>Random Forest are powerful predictive models.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-07-gradient-boosting"><p>Content from <a href="07-gradient-boosting.html">Gradient boosting</a></p>
<hr>
<p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/episodes/07-gradient-boosting.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is the state of the art in tree models?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Train gradient boosted models.</li>
<li>Visualise the decision boundaries.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="gradient-boosting">Gradient boosting<a class="anchor" aria-label="anchor" href="#gradient-boosting"></a>
</h2>
<hr class="half-width">
<p>Last, but not least, we move on to gradient boosting. Gradient
boosting, our last topic, elegantly combines concepts from the previous
methods. As a “boosting” method, gradient boosting involves iteratively
building trees, aiming to improve upon misclassifications of the
previous tree. Gradient boosting also borrows the concept of
sub-sampling the variables (just like Random Forests), which can help to
prevent overfitting.</p>
<p>While it is too much to express in this tutorial, the biggest
innovation in gradient boosting is that it provides a unifying
mathematical framework for boosting models. The approach explicitly
casts the problem of building a tree as an optimization problem,
defining mathematical functions for how well a tree is performing (which
we had before) and how complex a tree is. In this light, one can
actually treat AdaBoost as a “special case” of gradient boosting, where
the loss function is chosen to be the exponential loss.</p>
<p>Let’s build a gradient boosting model.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>np.random.seed(<span class="dv">321</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>mdl <span class="op">=</span> ensemble.GradientBoostingClassifier(n_estimators<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>mdl <span class="op">=</span> mdl.fit(x_train.values, y_train.values)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>[<span class="dv">9</span>,<span class="dv">5</span>])</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>txt <span class="op">=</span> <span class="st">'Gradient boosted tree (final decision surface)'</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>glowyr.plot_model_pred_2d(mdl, x_train, y_train, title<span class="op">=</span>txt)</span></code></pre>
</div>
<figure><img src="fig/section7-fig1.png" width="900" class="figure mx-auto d-block"></figure><div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>As a “boosting” method, gradient boosting involves iteratively
building trees, aiming to improve upon misclassifications of the
previous tree.</li>
<li>Gradient boosting also borrows the concept of sub-sampling the
variables (just like Random Forests), which can help to prevent
overfitting.</li>
<li>The performance gains come at the cost of interpretability.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-08-performance"><p>Content from <a href="08-performance.html">Performance</a></p>
<hr>
<p>Last updated on 2024-11-14 |

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/episodes/08-performance.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How well do our predictive models perform?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Evaluate the performance of our different models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="comparing-model-performance">Comparing model performance<a class="anchor" aria-label="anchor" href="#comparing-model-performance"></a>
</h2>
<hr class="half-width">
<p>We’ve now learned the basics of the various tree methods and have
visualized most of them. Let’s finish by comparing the performance of
our models on our held-out test data. Our goal, remember, is to predict
whether or not a patient will survive their hospital stay using the
patient’s age and acute physiology score computed on the first day of
their ICU stay.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>clf <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>clf[<span class="st">'Decision Tree'</span>] <span class="op">=</span> tree.DecisionTreeClassifier(criterion<span class="op">=</span><span class="st">'entropy'</span>, splitter<span class="op">=</span><span class="st">'best'</span>).fit(x_train.values, y_train.values)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>clf[<span class="st">'Gradient Boosting'</span>] <span class="op">=</span> ensemble.GradientBoostingClassifier(n_estimators<span class="op">=</span><span class="dv">10</span>).fit(x_train.values, y_train.values)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>clf[<span class="st">'Random Forest'</span>] <span class="op">=</span> ensemble.RandomForestClassifier(n_estimators<span class="op">=</span><span class="dv">10</span>).fit(x_train.values, y_train.values)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>clf[<span class="st">'Bagging'</span>] <span class="op">=</span>  ensemble.BaggingClassifier(n_estimators<span class="op">=</span><span class="dv">10</span>).fit(x_train.values, y_train.values)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>clf[<span class="st">'AdaBoost'</span>] <span class="op">=</span>  ensemble.AdaBoostClassifier(n_estimators<span class="op">=</span><span class="dv">10</span>).fit(x_train.values, y_train.values)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>[<span class="dv">10</span>,<span class="dv">10</span>])</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'AUROC</span><span class="ch">\t</span><span class="st">Model'</span>)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="cf">for</span> i, curr_mdl <span class="kw">in</span> <span class="bu">enumerate</span>(clf):    </span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>    yhat <span class="op">=</span> clf[curr_mdl].predict_proba(x_test.values)[:,<span class="dv">1</span>]</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>    score <span class="op">=</span> metrics.roc_auc_score(y_test, yhat)</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:0.3f}</span><span class="ch">\t</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(score, curr_mdl))</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">3</span>,<span class="dv">2</span>,i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>    glowyr. plot_model_pred_2d(clf[curr_mdl], x_test, y_test, title<span class="op">=</span>curr_mdl)</span></code></pre>
</div>
<figure><img src="fig/section8-fig1.png" width="900" class="figure mx-auto d-block"></figure><p>Here we can see that quantitatively, gradient boosting has produced
the highest discrimination among all the models (~0.91). You’ll see that
some of the models appear to have simpler decision surfaces, which tends
to result in improved generalization on a held-out test set (though not
always!).</p>
<p>To make appropriate comparisons, we should calculate 95% confidence
intervals on these performance estimates. This can be done a number of
ways. A simple but effective approach is to use bootstrapping, a
resampling technique. In bootstrapping, we generate multiple datasets
from the test set (allowing the same data point to be sampled multiple
times). Using these datasets, we can then estimate the confidence
intervals.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>There is a large performance gap between different types of
tree.</li>
<li>Boosted models typically perform strongly.</li>
</ul>
</div>
</div>
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/machine-learning-trees-python/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/machine-learning-trees-python/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:tpollard@mit.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.10" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.7" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/machine-learning-trees-python/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/machine-learning-trees-python/aio.html",
  "identifier": "https://carpentries-incubator.github.io/machine-learning-trees-python/aio.html",
  "dateCreated": "2021-10-22",
  "dateModified": "2024-12-31",
  "datePublished": "2024-12-31"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

